{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb32df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import moviepy.editor as mp\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf7093e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-40.785557</td>\n",
       "      <td>-31.903383</td>\n",
       "      <td>-28.509054</td>\n",
       "      <td>-24.394083</td>\n",
       "      <td>-23.567312</td>\n",
       "      <td>-28.972204</td>\n",
       "      <td>...</td>\n",
       "      <td>-57.298710</td>\n",
       "      <td>-56.888195</td>\n",
       "      <td>-57.355335</td>\n",
       "      <td>-57.385845</td>\n",
       "      <td>-58.302578</td>\n",
       "      <td>-59.111305</td>\n",
       "      <td>-59.915573</td>\n",
       "      <td>-61.931507</td>\n",
       "      <td>-67.367920</td>\n",
       "      <td>-75.377335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-46.091118</td>\n",
       "      <td>-41.264591</td>\n",
       "      <td>-34.173096</td>\n",
       "      <td>-27.214109</td>\n",
       "      <td>-26.786467</td>\n",
       "      <td>-31.770670</td>\n",
       "      <td>...</td>\n",
       "      <td>-62.016983</td>\n",
       "      <td>-63.054230</td>\n",
       "      <td>-63.258858</td>\n",
       "      <td>-62.921265</td>\n",
       "      <td>-64.128593</td>\n",
       "      <td>-64.542862</td>\n",
       "      <td>-64.874359</td>\n",
       "      <td>-65.916077</td>\n",
       "      <td>-69.985847</td>\n",
       "      <td>-76.369377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-49.231205</td>\n",
       "      <td>-42.710953</td>\n",
       "      <td>-39.304115</td>\n",
       "      <td>-34.735527</td>\n",
       "      <td>-34.517563</td>\n",
       "      <td>-38.112541</td>\n",
       "      <td>...</td>\n",
       "      <td>-64.425728</td>\n",
       "      <td>-64.667534</td>\n",
       "      <td>-65.593277</td>\n",
       "      <td>-65.887383</td>\n",
       "      <td>-66.269699</td>\n",
       "      <td>-67.071754</td>\n",
       "      <td>-67.932968</td>\n",
       "      <td>-69.560272</td>\n",
       "      <td>-74.179443</td>\n",
       "      <td>-79.334343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>neutral</td>\n",
       "      <td>-45.178123</td>\n",
       "      <td>-41.165024</td>\n",
       "      <td>-38.436745</td>\n",
       "      <td>-33.843090</td>\n",
       "      <td>-30.376688</td>\n",
       "      <td>-31.385689</td>\n",
       "      <td>...</td>\n",
       "      <td>-60.421524</td>\n",
       "      <td>-60.690266</td>\n",
       "      <td>-60.210945</td>\n",
       "      <td>-59.100220</td>\n",
       "      <td>-59.930977</td>\n",
       "      <td>-60.668087</td>\n",
       "      <td>-62.051445</td>\n",
       "      <td>-62.430584</td>\n",
       "      <td>-67.484306</td>\n",
       "      <td>-75.673409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>surprise</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>positive</td>\n",
       "      <td>-48.915817</td>\n",
       "      <td>-44.924217</td>\n",
       "      <td>-41.524128</td>\n",
       "      <td>-35.188869</td>\n",
       "      <td>-31.114756</td>\n",
       "      <td>-31.212828</td>\n",
       "      <td>...</td>\n",
       "      <td>-61.329353</td>\n",
       "      <td>-61.320774</td>\n",
       "      <td>-61.754005</td>\n",
       "      <td>-63.535156</td>\n",
       "      <td>-64.087585</td>\n",
       "      <td>-64.762703</td>\n",
       "      <td>-66.426491</td>\n",
       "      <td>-68.073280</td>\n",
       "      <td>-72.419991</td>\n",
       "      <td>-79.377052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12821</th>\n",
       "      <td>sadness</td>\n",
       "      <td>1030</td>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>-53.905170</td>\n",
       "      <td>-43.386887</td>\n",
       "      <td>-37.086166</td>\n",
       "      <td>-33.871140</td>\n",
       "      <td>-30.652399</td>\n",
       "      <td>-25.967619</td>\n",
       "      <td>...</td>\n",
       "      <td>-61.185417</td>\n",
       "      <td>-61.963745</td>\n",
       "      <td>-62.287804</td>\n",
       "      <td>-63.423450</td>\n",
       "      <td>-63.708729</td>\n",
       "      <td>-63.579025</td>\n",
       "      <td>-64.373978</td>\n",
       "      <td>-65.627625</td>\n",
       "      <td>-70.394920</td>\n",
       "      <td>-78.519653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12822</th>\n",
       "      <td>sadness</td>\n",
       "      <td>1036</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>-31.754524</td>\n",
       "      <td>-31.717495</td>\n",
       "      <td>-29.293173</td>\n",
       "      <td>-27.252337</td>\n",
       "      <td>-24.792559</td>\n",
       "      <td>-24.326172</td>\n",
       "      <td>...</td>\n",
       "      <td>-59.602276</td>\n",
       "      <td>-59.736683</td>\n",
       "      <td>-59.998604</td>\n",
       "      <td>-60.810440</td>\n",
       "      <td>-61.052074</td>\n",
       "      <td>-61.299347</td>\n",
       "      <td>-62.677368</td>\n",
       "      <td>-63.914894</td>\n",
       "      <td>-68.689537</td>\n",
       "      <td>-77.104012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12823</th>\n",
       "      <td>sadness</td>\n",
       "      <td>1036</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>-25.843838</td>\n",
       "      <td>-25.938826</td>\n",
       "      <td>-24.345428</td>\n",
       "      <td>-21.947716</td>\n",
       "      <td>-17.645626</td>\n",
       "      <td>-18.174150</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.014198</td>\n",
       "      <td>-56.561848</td>\n",
       "      <td>-56.946644</td>\n",
       "      <td>-58.137333</td>\n",
       "      <td>-58.426796</td>\n",
       "      <td>-58.635120</td>\n",
       "      <td>-60.210060</td>\n",
       "      <td>-61.658447</td>\n",
       "      <td>-66.239624</td>\n",
       "      <td>-76.923187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12824</th>\n",
       "      <td>sadness</td>\n",
       "      <td>1036</td>\n",
       "      <td>8</td>\n",
       "      <td>negative</td>\n",
       "      <td>-23.881714</td>\n",
       "      <td>-24.504099</td>\n",
       "      <td>-24.566311</td>\n",
       "      <td>-22.493364</td>\n",
       "      <td>-19.697737</td>\n",
       "      <td>-20.646078</td>\n",
       "      <td>...</td>\n",
       "      <td>-50.514225</td>\n",
       "      <td>-50.690666</td>\n",
       "      <td>-50.787540</td>\n",
       "      <td>-52.253510</td>\n",
       "      <td>-52.281872</td>\n",
       "      <td>-52.611309</td>\n",
       "      <td>-53.727940</td>\n",
       "      <td>-54.972492</td>\n",
       "      <td>-59.620949</td>\n",
       "      <td>-71.365517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12825</th>\n",
       "      <td>sadness</td>\n",
       "      <td>1036</td>\n",
       "      <td>20</td>\n",
       "      <td>negative</td>\n",
       "      <td>-26.704992</td>\n",
       "      <td>-23.456890</td>\n",
       "      <td>-22.470228</td>\n",
       "      <td>-20.334957</td>\n",
       "      <td>-19.231424</td>\n",
       "      <td>-21.924377</td>\n",
       "      <td>...</td>\n",
       "      <td>-56.759167</td>\n",
       "      <td>-56.929043</td>\n",
       "      <td>-57.230755</td>\n",
       "      <td>-58.707653</td>\n",
       "      <td>-59.147003</td>\n",
       "      <td>-59.157055</td>\n",
       "      <td>-60.734360</td>\n",
       "      <td>-62.214680</td>\n",
       "      <td>-66.878662</td>\n",
       "      <td>-77.405983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12826 rows Ã— 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Emotion  Dialogue_ID  Utterance_ID Sentiment          0          1  \\\n",
       "0       neutral            0             0   neutral -40.785557 -31.903383   \n",
       "1       neutral            0             1   neutral -46.091118 -41.264591   \n",
       "2       neutral            0             2   neutral -49.231205 -42.710953   \n",
       "3       neutral            0             3   neutral -45.178123 -41.165024   \n",
       "4      surprise            0             4  positive -48.915817 -44.924217   \n",
       "...         ...          ...           ...       ...        ...        ...   \n",
       "12821   sadness         1030             4  negative -53.905170 -43.386887   \n",
       "12822   sadness         1036             1  negative -31.754524 -31.717495   \n",
       "12823   sadness         1036             3  negative -25.843838 -25.938826   \n",
       "12824   sadness         1036             8  negative -23.881714 -24.504099   \n",
       "12825   sadness         1036            20  negative -26.704992 -23.456890   \n",
       "\n",
       "               2          3          4          5  ...        118        119  \\\n",
       "0     -28.509054 -24.394083 -23.567312 -28.972204  ... -57.298710 -56.888195   \n",
       "1     -34.173096 -27.214109 -26.786467 -31.770670  ... -62.016983 -63.054230   \n",
       "2     -39.304115 -34.735527 -34.517563 -38.112541  ... -64.425728 -64.667534   \n",
       "3     -38.436745 -33.843090 -30.376688 -31.385689  ... -60.421524 -60.690266   \n",
       "4     -41.524128 -35.188869 -31.114756 -31.212828  ... -61.329353 -61.320774   \n",
       "...          ...        ...        ...        ...  ...        ...        ...   \n",
       "12821 -37.086166 -33.871140 -30.652399 -25.967619  ... -61.185417 -61.963745   \n",
       "12822 -29.293173 -27.252337 -24.792559 -24.326172  ... -59.602276 -59.736683   \n",
       "12823 -24.345428 -21.947716 -17.645626 -18.174150  ... -56.014198 -56.561848   \n",
       "12824 -24.566311 -22.493364 -19.697737 -20.646078  ... -50.514225 -50.690666   \n",
       "12825 -22.470228 -20.334957 -19.231424 -21.924377  ... -56.759167 -56.929043   \n",
       "\n",
       "             120        121        122        123        124        125  \\\n",
       "0     -57.355335 -57.385845 -58.302578 -59.111305 -59.915573 -61.931507   \n",
       "1     -63.258858 -62.921265 -64.128593 -64.542862 -64.874359 -65.916077   \n",
       "2     -65.593277 -65.887383 -66.269699 -67.071754 -67.932968 -69.560272   \n",
       "3     -60.210945 -59.100220 -59.930977 -60.668087 -62.051445 -62.430584   \n",
       "4     -61.754005 -63.535156 -64.087585 -64.762703 -66.426491 -68.073280   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "12821 -62.287804 -63.423450 -63.708729 -63.579025 -64.373978 -65.627625   \n",
       "12822 -59.998604 -60.810440 -61.052074 -61.299347 -62.677368 -63.914894   \n",
       "12823 -56.946644 -58.137333 -58.426796 -58.635120 -60.210060 -61.658447   \n",
       "12824 -50.787540 -52.253510 -52.281872 -52.611309 -53.727940 -54.972492   \n",
       "12825 -57.230755 -58.707653 -59.147003 -59.157055 -60.734360 -62.214680   \n",
       "\n",
       "             126        127  \n",
       "0     -67.367920 -75.377335  \n",
       "1     -69.985847 -76.369377  \n",
       "2     -74.179443 -79.334343  \n",
       "3     -67.484306 -75.673409  \n",
       "4     -72.419991 -79.377052  \n",
       "...          ...        ...  \n",
       "12821 -70.394920 -78.519653  \n",
       "12822 -68.689537 -77.104012  \n",
       "12823 -66.239624 -76.923187  \n",
       "12824 -59.620949 -71.365517  \n",
       "12825 -66.878662 -77.405983  \n",
       "\n",
       "[12826 rows x 132 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('modified_df.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c01fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     4709\n",
       "joy         1743\n",
       "sadness     1366\n",
       "disgust     1355\n",
       "fear        1340\n",
       "surprise    1204\n",
       "anger       1109\n",
       "Name: Emotion, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0d68cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "anger_df = df[df['Emotion']=='anger']\n",
    "df = df.append(anger_df.sample(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87785ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['Emotion','Sentiment', 'Dialogue_ID', 'Utterance_ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e810d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'neutral', 'neutral', ..., 'sadness', 'sadness',\n",
       "       'sadness'], dtype='<U8')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(df['Emotion'].tolist())\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d147421f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-40.78555679, -31.90338326, -28.50905418, ..., -61.93150711,\n",
       "        -67.36791992, -75.37733459],\n",
       "       [-46.09111786, -41.26459122, -34.1730957 , ..., -65.91607666,\n",
       "        -69.98584747, -76.36937714],\n",
       "       [-49.23120499, -42.71095276, -39.3041153 , ..., -69.56027222,\n",
       "        -74.17944336, -79.33434296],\n",
       "       ...,\n",
       "       [-25.84383774, -25.93882561, -24.34542847, ..., -61.65844727,\n",
       "        -66.23962402, -76.92318726],\n",
       "       [-23.88171387, -24.50409889, -24.56631088, ..., -54.97249222,\n",
       "        -59.62094879, -71.36551666],\n",
       "       [-26.70499229, -23.45689011, -22.4702282 , ..., -62.21467972,\n",
       "        -66.87866211, -77.40598297]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(features)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75201473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = LabelEncoder()\n",
    "label_y = to_categorical(lb.fit_transform(y))\n",
    "label_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f847bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness',\n",
       "       'surprise'], dtype='<U8')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5556657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, label_y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9343750e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10260, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73f595d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d44f7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70a99f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_8 (Conv1D)            (None, 128, 256)          2304      \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 128, 256)          524544    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 16, 128)           262272    \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 16, 128)           131200    \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 16, 128)           131200    \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 16, 128)           131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 128)           512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 2, 64)             65600     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 2, 64)             32832     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               328704    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 1799      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 1,677,959\n",
      "Trainable params: 1,677,703\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))\n",
    "\n",
    "model.add(Conv1D(256, 8, padding='same', activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "\n",
    "model.add(Conv1D(128, 8, padding='same', activation=\"relu\"))\n",
    "model.add(Conv1D(128, 8, padding='same', activation=\"relu\"))\n",
    "\n",
    "model.add(Conv1D(128, 8, padding='same', activation=\"relu\"))\n",
    "\n",
    "model.add(Conv1D(128, 8, padding='same', activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "\n",
    "model.add(Conv1D(64, 8, padding='same', activation=\"relu\"))\n",
    "\n",
    "model.add(Conv1D(64, 8, padding='same', activation=\"relu\"))\n",
    "\n",
    "    #model.add(Flatten())\n",
    "\n",
    "model.add(layers.LSTM(256))\n",
    "\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(7)) \n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5187026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 1s 18ms/step - loss: 1.9411 - accuracy: 0.1083\n",
      "Pre-training accuracy: 10.8340%\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63169363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.8002 - accuracy: 0.3645\n",
      "Epoch 00001: val_loss improved from inf to 1.89924, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 29s 701ms/step - loss: 1.8002 - accuracy: 0.3645 - val_loss: 1.8992 - val_accuracy: 0.3628\n",
      "Epoch 2/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7892 - accuracy: 0.3682\n",
      "Epoch 00002: val_loss improved from 1.89924 to 1.81254, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 28s 687ms/step - loss: 1.7892 - accuracy: 0.3682 - val_loss: 1.8125 - val_accuracy: 0.3628\n",
      "Epoch 3/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7853 - accuracy: 0.3682\n",
      "Epoch 00003: val_loss improved from 1.81254 to 1.79950, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 28s 677ms/step - loss: 1.7853 - accuracy: 0.3682 - val_loss: 1.7995 - val_accuracy: 0.3628\n",
      "Epoch 4/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7851 - accuracy: 0.3682\n",
      "Epoch 00004: val_loss did not improve from 1.79950\n",
      "41/41 [==============================] - 28s 672ms/step - loss: 1.7851 - accuracy: 0.3682 - val_loss: 1.8019 - val_accuracy: 0.3628\n",
      "Epoch 5/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7852 - accuracy: 0.3682\n",
      "Epoch 00005: val_loss improved from 1.79950 to 1.79365, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 29s 709ms/step - loss: 1.7852 - accuracy: 0.3682 - val_loss: 1.7936 - val_accuracy: 0.3628\n",
      "Epoch 6/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7853 - accuracy: 0.3682\n",
      "Epoch 00006: val_loss improved from 1.79365 to 1.78894, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 29s 710ms/step - loss: 1.7853 - accuracy: 0.3682 - val_loss: 1.7889 - val_accuracy: 0.3628\n",
      "Epoch 7/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7853 - accuracy: 0.3682\n",
      "Epoch 00007: val_loss did not improve from 1.78894\n",
      "41/41 [==============================] - 29s 701ms/step - loss: 1.7853 - accuracy: 0.3682 - val_loss: 1.7916 - val_accuracy: 0.3628\n",
      "Epoch 8/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7842 - accuracy: 0.3682\n",
      "Epoch 00008: val_loss did not improve from 1.78894\n",
      "41/41 [==============================] - 29s 704ms/step - loss: 1.7842 - accuracy: 0.3682 - val_loss: 1.7933 - val_accuracy: 0.3628\n",
      "Epoch 9/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7858 - accuracy: 0.3682\n",
      "Epoch 00009: val_loss did not improve from 1.78894\n",
      "41/41 [==============================] - 29s 697ms/step - loss: 1.7858 - accuracy: 0.3682 - val_loss: 1.8138 - val_accuracy: 0.3628\n",
      "Epoch 10/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7815 - accuracy: 0.3682\n",
      "Epoch 00010: val_loss improved from 1.78894 to 1.78804, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 29s 698ms/step - loss: 1.7815 - accuracy: 0.3682 - val_loss: 1.7880 - val_accuracy: 0.3628\n",
      "Epoch 11/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7800 - accuracy: 0.3682\n",
      "Epoch 00011: val_loss did not improve from 1.78804\n",
      "41/41 [==============================] - 28s 682ms/step - loss: 1.7800 - accuracy: 0.3682 - val_loss: 2.4319 - val_accuracy: 0.3628\n",
      "Epoch 12/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7793 - accuracy: 0.3682\n",
      "Epoch 00012: val_loss did not improve from 1.78804\n",
      "41/41 [==============================] - 28s 674ms/step - loss: 1.7793 - accuracy: 0.3682 - val_loss: 1.9313 - val_accuracy: 0.3628\n",
      "Epoch 13/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7772 - accuracy: 0.3682\n",
      "Epoch 00013: val_loss did not improve from 1.78804\n",
      "41/41 [==============================] - 28s 673ms/step - loss: 1.7772 - accuracy: 0.3682 - val_loss: 2.6844 - val_accuracy: 0.3628\n",
      "Epoch 14/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7680 - accuracy: 0.3682\n",
      "Epoch 00014: val_loss did not improve from 1.78804\n",
      "41/41 [==============================] - 28s 677ms/step - loss: 1.7680 - accuracy: 0.3682 - val_loss: 2.0381 - val_accuracy: 0.3262\n",
      "Epoch 15/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7649 - accuracy: 0.3680\n",
      "Epoch 00015: val_loss did not improve from 1.78804\n",
      "41/41 [==============================] - 28s 687ms/step - loss: 1.7649 - accuracy: 0.3680 - val_loss: 3.1755 - val_accuracy: 0.1150\n",
      "Epoch 16/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7634 - accuracy: 0.3683\n",
      "Epoch 00016: val_loss did not improve from 1.78804\n",
      "41/41 [==============================] - 28s 675ms/step - loss: 1.7634 - accuracy: 0.3683 - val_loss: 1.8702 - val_accuracy: 0.3597\n",
      "Epoch 17/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7627 - accuracy: 0.3671\n",
      "Epoch 00017: val_loss did not improve from 1.78804\n",
      "41/41 [==============================] - 28s 672ms/step - loss: 1.7627 - accuracy: 0.3671 - val_loss: 3.9408 - val_accuracy: 0.0998\n",
      "Epoch 18/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7614 - accuracy: 0.3697\n",
      "Epoch 00018: val_loss did not improve from 1.78804\n",
      "41/41 [==============================] - 28s 676ms/step - loss: 1.7614 - accuracy: 0.3697 - val_loss: 1.9380 - val_accuracy: 0.3429\n",
      "Epoch 19/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7582 - accuracy: 0.3679\n",
      "Epoch 00019: val_loss did not improve from 1.78804\n",
      "41/41 [==============================] - 28s 686ms/step - loss: 1.7582 - accuracy: 0.3679 - val_loss: 1.8349 - val_accuracy: 0.3628\n",
      "Epoch 20/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7631 - accuracy: 0.3685\n",
      "Epoch 00020: val_loss did not improve from 1.78804\n",
      "41/41 [==============================] - 28s 689ms/step - loss: 1.7631 - accuracy: 0.3685 - val_loss: 1.9456 - val_accuracy: 0.3609\n",
      "Epoch 21/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7507 - accuracy: 0.3706\n",
      "Epoch 00021: val_loss did not improve from 1.78804\n",
      "41/41 [==============================] - 28s 694ms/step - loss: 1.7507 - accuracy: 0.3706 - val_loss: 1.8176 - val_accuracy: 0.3597\n",
      "Epoch 22/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7597 - accuracy: 0.3706\n",
      "Epoch 00022: val_loss improved from 1.78804 to 1.78689, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 28s 694ms/step - loss: 1.7597 - accuracy: 0.3706 - val_loss: 1.7869 - val_accuracy: 0.3624\n",
      "Epoch 23/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7487 - accuracy: 0.3704\n",
      "Epoch 00023: val_loss did not improve from 1.78689\n",
      "41/41 [==============================] - 28s 692ms/step - loss: 1.7487 - accuracy: 0.3704 - val_loss: 1.8912 - val_accuracy: 0.3371\n",
      "Epoch 24/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7444 - accuracy: 0.3697\n",
      "Epoch 00024: val_loss did not improve from 1.78689\n",
      "41/41 [==============================] - 27s 660ms/step - loss: 1.7444 - accuracy: 0.3697 - val_loss: 1.7929 - val_accuracy: 0.3589\n",
      "Epoch 25/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7426 - accuracy: 0.3719\n",
      "Epoch 00025: val_loss did not improve from 1.78689\n",
      "41/41 [==============================] - 27s 667ms/step - loss: 1.7426 - accuracy: 0.3719 - val_loss: 1.8013 - val_accuracy: 0.3581\n",
      "Epoch 26/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7379 - accuracy: 0.3734\n",
      "Epoch 00026: val_loss improved from 1.78689 to 1.77752, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 28s 675ms/step - loss: 1.7379 - accuracy: 0.3734 - val_loss: 1.7775 - val_accuracy: 0.3659\n",
      "Epoch 27/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7427 - accuracy: 0.3712\n",
      "Epoch 00027: val_loss improved from 1.77752 to 1.75716, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 27s 662ms/step - loss: 1.7427 - accuracy: 0.3712 - val_loss: 1.7572 - val_accuracy: 0.3655\n",
      "Epoch 28/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7416 - accuracy: 0.3720\n",
      "Epoch 00028: val_loss did not improve from 1.75716\n",
      "41/41 [==============================] - 27s 656ms/step - loss: 1.7416 - accuracy: 0.3720 - val_loss: 1.7726 - val_accuracy: 0.3636\n",
      "Epoch 29/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7377 - accuracy: 0.3722\n",
      "Epoch 00029: val_loss did not improve from 1.75716\n",
      "41/41 [==============================] - 27s 658ms/step - loss: 1.7377 - accuracy: 0.3722 - val_loss: 1.7658 - val_accuracy: 0.3578\n",
      "Epoch 30/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7308 - accuracy: 0.3722\n",
      "Epoch 00030: val_loss improved from 1.75716 to 1.74996, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 27s 658ms/step - loss: 1.7308 - accuracy: 0.3722 - val_loss: 1.7500 - val_accuracy: 0.3632\n",
      "Epoch 31/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7300 - accuracy: 0.3733\n",
      "Epoch 00031: val_loss did not improve from 1.74996\n",
      "41/41 [==============================] - 27s 660ms/step - loss: 1.7300 - accuracy: 0.3733 - val_loss: 1.7638 - val_accuracy: 0.3620\n",
      "Epoch 32/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7300 - accuracy: 0.3719\n",
      "Epoch 00032: val_loss did not improve from 1.74996\n",
      "41/41 [==============================] - 27s 657ms/step - loss: 1.7300 - accuracy: 0.3719 - val_loss: 1.7623 - val_accuracy: 0.3640\n",
      "Epoch 33/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7249 - accuracy: 0.3726\n",
      "Epoch 00033: val_loss did not improve from 1.74996\n",
      "41/41 [==============================] - 27s 658ms/step - loss: 1.7249 - accuracy: 0.3726 - val_loss: 1.7663 - val_accuracy: 0.3667\n",
      "Epoch 34/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7184 - accuracy: 0.3762\n",
      "Epoch 00034: val_loss did not improve from 1.74996\n",
      "41/41 [==============================] - 27s 655ms/step - loss: 1.7184 - accuracy: 0.3762 - val_loss: 1.8549 - val_accuracy: 0.3500\n",
      "Epoch 35/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7118 - accuracy: 0.3771\n",
      "Epoch 00035: val_loss improved from 1.74996 to 1.74559, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 27s 664ms/step - loss: 1.7118 - accuracy: 0.3771 - val_loss: 1.7456 - val_accuracy: 0.3648\n",
      "Epoch 36/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7075 - accuracy: 0.3753\n",
      "Epoch 00036: val_loss did not improve from 1.74559\n",
      "41/41 [==============================] - 27s 661ms/step - loss: 1.7075 - accuracy: 0.3753 - val_loss: 1.8520 - val_accuracy: 0.3535\n",
      "Epoch 37/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6993 - accuracy: 0.3804\n",
      "Epoch 00037: val_loss improved from 1.74559 to 1.72589, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 27s 660ms/step - loss: 1.6993 - accuracy: 0.3804 - val_loss: 1.7259 - val_accuracy: 0.3706\n",
      "Epoch 38/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6936 - accuracy: 0.3790\n",
      "Epoch 00038: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 659ms/step - loss: 1.6936 - accuracy: 0.3790 - val_loss: 1.7627 - val_accuracy: 0.3632\n",
      "Epoch 39/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6868 - accuracy: 0.3839\n",
      "Epoch 00039: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 665ms/step - loss: 1.6868 - accuracy: 0.3839 - val_loss: 1.8599 - val_accuracy: 0.3605\n",
      "Epoch 40/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7495 - accuracy: 0.3703\n",
      "Epoch 00040: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 658ms/step - loss: 1.7495 - accuracy: 0.3703 - val_loss: 1.8655 - val_accuracy: 0.3426\n",
      "Epoch 41/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7361 - accuracy: 0.3724\n",
      "Epoch 00041: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 657ms/step - loss: 1.7361 - accuracy: 0.3724 - val_loss: 2.3245 - val_accuracy: 0.2923\n",
      "Epoch 42/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.7066 - accuracy: 0.3796\n",
      "Epoch 00042: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 659ms/step - loss: 1.7066 - accuracy: 0.3796 - val_loss: 1.9881 - val_accuracy: 0.3554\n",
      "Epoch 43/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6901 - accuracy: 0.3822\n",
      "Epoch 00043: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 658ms/step - loss: 1.6901 - accuracy: 0.3822 - val_loss: 2.0944 - val_accuracy: 0.3406\n",
      "Epoch 44/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6907 - accuracy: 0.3794\n",
      "Epoch 00044: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 659ms/step - loss: 1.6907 - accuracy: 0.3794 - val_loss: 2.1438 - val_accuracy: 0.3449\n",
      "Epoch 45/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6740 - accuracy: 0.3850\n",
      "Epoch 00045: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 660ms/step - loss: 1.6740 - accuracy: 0.3850 - val_loss: 2.6682 - val_accuracy: 0.3079\n",
      "Epoch 46/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6859 - accuracy: 0.3793\n",
      "Epoch 00046: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 661ms/step - loss: 1.6859 - accuracy: 0.3793 - val_loss: 2.0578 - val_accuracy: 0.3387\n",
      "Epoch 47/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6662 - accuracy: 0.3874\n",
      "Epoch 00047: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 662ms/step - loss: 1.6662 - accuracy: 0.3874 - val_loss: 1.7527 - val_accuracy: 0.3675\n",
      "Epoch 48/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6513 - accuracy: 0.3893\n",
      "Epoch 00048: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 657ms/step - loss: 1.6513 - accuracy: 0.3893 - val_loss: 2.0747 - val_accuracy: 0.3504\n",
      "Epoch 49/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6349 - accuracy: 0.3957\n",
      "Epoch 00049: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 657ms/step - loss: 1.6349 - accuracy: 0.3957 - val_loss: 2.0427 - val_accuracy: 0.3375\n",
      "Epoch 50/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6249 - accuracy: 0.3987\n",
      "Epoch 00050: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 656ms/step - loss: 1.6249 - accuracy: 0.3987 - val_loss: 2.1004 - val_accuracy: 0.3352\n",
      "Epoch 51/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6205 - accuracy: 0.3985\n",
      "Epoch 00051: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 664ms/step - loss: 1.6205 - accuracy: 0.3985 - val_loss: 2.1521 - val_accuracy: 0.3453\n",
      "Epoch 52/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.6037 - accuracy: 0.4042\n",
      "Epoch 00052: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 663ms/step - loss: 1.6037 - accuracy: 0.4042 - val_loss: 2.3406 - val_accuracy: 0.3367\n",
      "Epoch 53/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.5954 - accuracy: 0.4094\n",
      "Epoch 00053: val_loss did not improve from 1.72589\n",
      "41/41 [==============================] - 27s 658ms/step - loss: 1.5954 - accuracy: 0.4094 - val_loss: 2.5159 - val_accuracy: 0.3305\n",
      "Epoch 54/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.5868 - accuracy: 0.4079\n",
      "Epoch 00054: val_loss improved from 1.72589 to 1.69905, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 27s 658ms/step - loss: 1.5868 - accuracy: 0.4079 - val_loss: 1.6990 - val_accuracy: 0.3749\n",
      "Epoch 55/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.5568 - accuracy: 0.4154\n",
      "Epoch 00055: val_loss did not improve from 1.69905\n",
      "41/41 [==============================] - 27s 657ms/step - loss: 1.5568 - accuracy: 0.4154 - val_loss: 2.6674 - val_accuracy: 0.3145\n",
      "Epoch 56/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.5528 - accuracy: 0.4232\n",
      "Epoch 00056: val_loss did not improve from 1.69905\n",
      "41/41 [==============================] - 27s 658ms/step - loss: 1.5528 - accuracy: 0.4232 - val_loss: 2.7353 - val_accuracy: 0.3523\n",
      "Epoch 57/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.5371 - accuracy: 0.4246\n",
      "Epoch 00057: val_loss did not improve from 1.69905\n",
      "41/41 [==============================] - 27s 658ms/step - loss: 1.5371 - accuracy: 0.4246 - val_loss: 2.9195 - val_accuracy: 0.3258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.5254 - accuracy: 0.4321\n",
      "Epoch 00058: val_loss did not improve from 1.69905\n",
      "41/41 [==============================] - 27s 666ms/step - loss: 1.5254 - accuracy: 0.4321 - val_loss: 1.8953 - val_accuracy: 0.3745\n",
      "Epoch 59/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.5084 - accuracy: 0.4343\n",
      "Epoch 00059: val_loss did not improve from 1.69905\n",
      "41/41 [==============================] - 27s 659ms/step - loss: 1.5084 - accuracy: 0.4343 - val_loss: 2.2185 - val_accuracy: 0.3523\n",
      "Epoch 60/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.4995 - accuracy: 0.4369\n",
      "Epoch 00060: val_loss improved from 1.69905 to 1.63619, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 27s 661ms/step - loss: 1.4995 - accuracy: 0.4369 - val_loss: 1.6362 - val_accuracy: 0.4053\n",
      "Epoch 61/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.4613 - accuracy: 0.4518\n",
      "Epoch 00061: val_loss did not improve from 1.63619\n",
      "41/41 [==============================] - 28s 675ms/step - loss: 1.4613 - accuracy: 0.4518 - val_loss: 3.0339 - val_accuracy: 0.2818\n",
      "Epoch 62/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.4600 - accuracy: 0.4515\n",
      "Epoch 00062: val_loss did not improve from 1.63619\n",
      "41/41 [==============================] - 28s 690ms/step - loss: 1.4600 - accuracy: 0.4515 - val_loss: 2.6981 - val_accuracy: 0.3289\n",
      "Epoch 63/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.4298 - accuracy: 0.4623\n",
      "Epoch 00063: val_loss did not improve from 1.63619\n",
      "41/41 [==============================] - 28s 686ms/step - loss: 1.4298 - accuracy: 0.4623 - val_loss: 2.5630 - val_accuracy: 0.3542\n",
      "Epoch 64/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.4414 - accuracy: 0.4582\n",
      "Epoch 00064: val_loss did not improve from 1.63619\n",
      "41/41 [==============================] - 27s 663ms/step - loss: 1.4414 - accuracy: 0.4582 - val_loss: 2.9064 - val_accuracy: 0.3188\n",
      "Epoch 65/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.4184 - accuracy: 0.4662\n",
      "Epoch 00065: val_loss did not improve from 1.63619\n",
      "41/41 [==============================] - 27s 655ms/step - loss: 1.4184 - accuracy: 0.4662 - val_loss: 2.6749 - val_accuracy: 0.3320\n",
      "Epoch 66/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.3862 - accuracy: 0.4765\n",
      "Epoch 00066: val_loss did not improve from 1.63619\n",
      "41/41 [==============================] - 27s 659ms/step - loss: 1.3862 - accuracy: 0.4765 - val_loss: 3.6164 - val_accuracy: 0.3071\n",
      "Epoch 67/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.3899 - accuracy: 0.4766\n",
      "Epoch 00067: val_loss did not improve from 1.63619\n",
      "41/41 [==============================] - 27s 657ms/step - loss: 1.3899 - accuracy: 0.4766 - val_loss: 2.9596 - val_accuracy: 0.3328\n",
      "Epoch 68/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.3676 - accuracy: 0.4850\n",
      "Epoch 00068: val_loss did not improve from 1.63619\n",
      "41/41 [==============================] - 27s 658ms/step - loss: 1.3676 - accuracy: 0.4850 - val_loss: 3.3156 - val_accuracy: 0.3133\n",
      "Epoch 69/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.3300 - accuracy: 0.4966\n",
      "Epoch 00069: val_loss did not improve from 1.63619\n",
      "41/41 [==============================] - 27s 657ms/step - loss: 1.3300 - accuracy: 0.4966 - val_loss: 2.2131 - val_accuracy: 0.3698\n",
      "Epoch 70/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.3470 - accuracy: 0.4928\n",
      "Epoch 00070: val_loss did not improve from 1.63619\n",
      "41/41 [==============================] - 27s 656ms/step - loss: 1.3470 - accuracy: 0.4928 - val_loss: 4.1260 - val_accuracy: 0.2747\n",
      "Epoch 71/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.3069 - accuracy: 0.5074\n",
      "Epoch 00071: val_loss did not improve from 1.63619\n",
      "41/41 [==============================] - 27s 655ms/step - loss: 1.3069 - accuracy: 0.5074 - val_loss: 2.0304 - val_accuracy: 0.3850\n",
      "Epoch 72/72\n",
      "41/41 [==============================] - ETA: 0s - loss: 1.2741 - accuracy: 0.5238\n",
      "Epoch 00072: val_loss improved from 1.63619 to 1.58755, saving model to saved_model_LSTM.hdf5\n",
      "41/41 [==============================] - 27s 659ms/step - loss: 1.2741 - accuracy: 0.5238 - val_loss: 1.5876 - val_accuracy: 0.4267\n",
      "Training completed in time:  0:33:48.681622\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 72\n",
    "num_batch_size = 256\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_model_LSTM.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c3605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
